<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Tung D. Le</title>
    <meta name="author" content="Tung D. Le" />
    <link rel="stylesheet" type="text/css" href="org.css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-2420289-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-2420289-4');
    </script>
  </head>
  <body>
<h1 id="tung-d.-le"><a href="https://www.ibm.biz/leductung">Tung D. Le</a></h1>
<p style="text-align: center;">
Senior Research Scientist <br> <a href="https://www.research.ibm.com/labs/tokyo/">IBM Research - Tokyo</a> <br> <a href="http://www.ibm.biz/leductung">http://www.ibm.biz/leductung</a>
</p>
<p><br></p>
<h2 id="research-interest">research interest</h2>
<hr />
<p>My main interest lies in the intersection of parallel programming, compiler, and deep learning. In particular, I would love to propose systematic optimizations for parallel programming and AI systems.</p>
<h2 id="academic-activities">academic activities</h2>
<hr />
<ul>
<li><a href="https://search.ieice.org/bin/editorial_board.php?lang=ed">Associate editor for IEICE transactions on Information and Systems (June 2020 - June 2024)</a></li>
<li>PC members:
<ul>
<li>ScalCom (<a href="http://www.smart-world.org/2018/scalcom/">2018</a>, <a href="http://www.smart-world.org/2019/scalcom/">2019</a>, 2020, 2021)</li>
<li>SoICT <a href="https://soict.org/2022/committees/program-committees/">2022</a></li>
</ul></li>
<li>Lectures: <a href="https://www.ocw.titech.ac.jp/index.php?module=General&amp;action=T0300&amp;GakubuCD=4&amp;GakkaCD=342222&amp;KeiCD=22&amp;course=22&amp;KougiCD=202204021&amp;Nendo=2022&amp;lang=EN&amp;vid=03">Deep Learning Compiler &amp; MLIR</a> at Tokyo Institue of Tecnology, Japan in 2023.</li>
<li><a href="https://dl.acm.org/author_page.cfm?id=84758683357">ACM Senior Member since October 2021</a></li>
</ul>
<h2 id="professional-experience">professional experience</h2>
<ul>
<li>7/2025 - now: senior research scientist at IBM Research - Tokyo, Japan.</li>
<li>5/2019 - 6/2025: staff research scientist at IBM Research - Tokyo, Japan.</li>
<li>1/2017 - 4/2019: research scientist at IBM Research - Tokyo, Japan.</li>
<li>4/2016 - 12/2016: postdoctoral researcher at IBM Research - Tokyo, Japan.</li>
<li>4/2013 - 3/2016: Ph.D. student at National Informatics Institue, Japan.
<ul>
<li>Topic: A systematic approach to regular-expression-based queries on big graphs</li>
<li>Supervisor: Prof. <a href="http://research.nii.ac.jp/~hu/">Zhenjiang Hu</a>.</li>
</ul></li>
<li>10/2012 - 3/2013: internship at National Informatics Institue, Japan.
<ul>
<li>Topic: Systematic parallel programming with MapReduce programming model.</li>
<li>Supervisor: Prof. <a href="http://research.nii.ac.jp/~hu/">Zhenjiang Hu</a>.</li>
</ul></li>
<li>4/2010 - 9/2010: internship at National Informatics Institue, Japan.
<ul>
<li>Topic: Skeleton parallel programming for Hierarchically Tiled Arrays.</li>
<li>Supervisor: Prof. <a href="http://research.nii.ac.jp/~hu/">Zhenjiang Hu</a>.</li>
</ul></li>
<li>10/2008 - 9/2010: M.Sc. student at <a href="https://www.hust.edu.vn/">HUST</a>.
<ul>
<li>Topic: Skeleton parallel programming for Hierarchically Tiled Arrays.</li>
<li>Supervisors: <a href="https://scholar.google.com/citations?user=PphDl-kAAAAJ&amp;hl=en">Dr. Huu-Duc Nguyen</a></li>
</ul></li>
<li>7/2007: two-week internship at <a href="https://www.sdsc.edu/">The San Diego Supercomputer Center, UCSD</a>.<br />
</li>
<li>6/2007 - 9/2012: Researcher at The High Peformance Computing Center, <a href="https://www.hust.edu.vn/">HUST</a>.</li>
</ul>
<h2 id="education">education</h2>
<hr />
<table>
<thead>
<tr class="header">
<th style="text-align: left;">When</th>
<th style="text-align: left;">What (in computer science) @ Where</th>
<th style="text-align: left;">With whom</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">2013 - 2016</td>
<td style="text-align: left;">Ph.D. @ <a href="http://www.nii.ac.jp/">SOKENDAI/NII, Japan</a></td>
<td style="text-align: left;"><a href="https://scholar.google.com/citations?user=MvGKdLoAAAAJ&amp;hl=en">Prof. Zhenjiang Hu</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">2008 - 2010</td>
<td style="text-align: left;">M.Sc. @ <a href="http://hust.edu.vn/">HUST, Vietnam</a></td>
<td style="text-align: left;"><a href="https://soict.hust.edu.vn/ts-nguyen-huu-duc.html">Dr. Huu-Duc Nguyen</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">2002 - 2007</td>
<td style="text-align: left;">B.S. @ <a href="http://hust.edu.vn/">HUST, Vietnam</a></td>
<td style="text-align: left;"><a href="https://uet.vnu.edu.vn/~nguyenthanhthuy/">Prof. Thanh-Thuy Nguyen</a></td>
</tr>
</tbody>
</table>
<h2 id="awards">awards</h2>
<hr />
<ul>
<li>IBM Corporate Techinical Award (CTA) in 2023.
<ul>
<li>The award recognizes the most outstanding IBM technical accomplishments in a year.</li>
<li>I got the award for my contribution to a deep learning compiler that supports IBM Integrated On-Chip AI Accelerator in IBM z16.</li>
</ul></li>
<li>IBM Tech2023 award for top technical contributors in IBM.</li>
<li>The IBM Outstanding Innovation Award (OIA) in 2022, 2025.</li>
<li>The IBM Outstanding Technical Achievement Award (OTAA) in 2018, 2019, 2020, 2021, 2024.</li>
<li>The IBM Technical Collaboration Achievement Program award (TCAP, a new type of OSRP) in 2022 (leader level), 2023 (leader level) and 2024 (leader level).</li>
<li>The IBM Open Source Recognition Program award (OSRP) in 2018 (leader level), 2019 (contributor level), 2020 (contributor level), 2021 (contributor level).</li>
<li><a href="http://www.hitachi-zaidan.net/global/scholarship/index.html">Hitachi Scholarship for Ph.D. courses</a> (October 2012 - April 2016). I am the only person from <a href="http://hust.edu.vn/">HUST, Vietnam</a> ever who gets the scholarship.</li>
<li>PLMW Scholarship 2015 to attend <a href="http://plmw15.iisc-seal.net">PLMW2015</a> and <a href="http://popl.mpi-sws.org/2015/">POPL2015</a>.</li>
<li><a href="http://www.icde2015.kr/info/student-travel-awards.html">ICDE2015 Student Travel Award</a>.</li>
</ul>
<h2 id="projects">projects</h2>
<hr />
<ul>
<li>2019 - Now: <a href="https://github.com/onnx/onnx-mlir">ONNX Models in MLIR Compiler Infrastructure</a></li>
<li>2016 - 2019: <a href="https://developer.ibm.com/components/ibm-power-ai/">IBM PowerAI</a>
<ul>
<li>Optimized deep learning frameworks Caffe, Chainer on POWER machines.</li>
<li>Proposed large model support for tensorflow (TFLMS). Open source was published at <a href="https://github.com/IBM/tensorflow-large-model-support/tree/tflmsv2">Graph-based Large Model Support for TensorFlow</a></li>
</ul></li>
</ul>
<h2 id="patents">patents</h2>
<hr />
<ol type="1">
<li><p><strong>Tung D. Le</strong>. <em>Neural programmer interpreters with modeled primitives</em>. US patent no. 11836613 B2, date of patent Dec. 5, 2023.</p></li>
<li><p>Gradus Janssen, Vladimir Zolotov, and <strong>Tung D. Le</strong>. <em>Neural network training using a data flow graph and dynamic memory management</em>. US patent no. 11512062 B2, date of patent Dec. 6, 2022.</p></li>
<li><p>Taro Sekiyama, Kiyokuni Kawachiya, <strong>Tung D. Le</strong>, and Yasushi Negishi. <em>Real-time resource usage reduction in artificial neural networks</em>. US patent no. 11461637 B2, date of patent Oct. 4, 2022.</p></li>
<li><p>Yasushi Negishi, <strong>Tung D. Le</strong>, Haruki Imai, and Kiyokuni Kawachiya. <em>Relu compression to reduce GPU memory</em>. US patent no. 11362670 B2, date of patent Jun. 14, 2022.</p></li>
<li><p><strong>Tung D. Le</strong>, Haruki Imai, Taro Sekiyama, and Yasushi Negishi. <em>Multi-GPU deep learning using CPUs</em>. US patent No. 11164079 B2, date of patent Nov. 2, 2021.</p></li>
<li><p><strong>Tung D. Le</strong> and Taro Sekiyama. <em>Localizing tree-based convolutional neural networks</em>. US patent no. 11106970 B2, date of patent Aug. 31, 2021.</p></li>
<li><p><strong>Tung D. Le</strong>, Imai Haruki, and Yasushi Negishi. <em>Efficient parallel training of a network model on multiple graphics processing units</em>. US patent no. 10949746 B2, date of patent Mar. 16, 2021.</p></li>
<li><p><strong>Tung D. Le</strong>, Haruki Imai, Yasushi Negishi, and Kiyokuni Kawachiya. <em>Graph rewriting for large model support using categorized topological sort</em>. US patent no. 10884755 B1, date of Pattent Jan. 5, 2021.</p></li>
<li><p>Taro Sekiyama, Kiyokuni Kawachiya, <strong>Tung D. Le</strong>, and Yasushi Negishi. <em>Real-time resource usage reduction in artificial neural networks</em>. US patent no. 10558914 B2, date of patent Feb. 11, 2020.</p></li>
<li><p>Taro Sekiyama, Kiyokuni Kawachiya, <strong>Tung D. Le</strong>, and Yasushi Negishi. <em>Real-time resource usage reduction in artificial neural networks</em>. US patent no. 10268951 B2, date of patent Apr. 23, 2019.</p></li>
</ol>
<h2 id="talks">talks</h2>
<hr />
<ol type="1">
<li><p><strong>Tung D. Le</strong> (speaker), Alexander Eichenberger, and Tong Chen. <em>Dynamic Dimension Analysis in onnx-mlir Compiler</em>. ONNX Community Meetup 2023 @ NVIDIA, Santa Clara, CA. June 28, 2023. <a href="https://wiki.lfaidata.foundation/display/DL/ONNX+Community+Day+2023+-+June+28?preview=/84705448/84705505/onnx-mlir-dynamic-dimension-analysis.pdf">Slides</a>. <a href="https://www.youtube.com/watch?v=wAnXCFh3lB8&amp;t=136s">Video</a>.</p></li>
<li><p><strong>Tung D. Le</strong> (speaker). <em>Onnx-mlir: an MLIR-based Compiler for ONNX Models - The Latest Status</em>. ONNX Community Meetup 2022 @ Microsoft Sillicon Valley Campus. June 28, 2022. <a href="https://wiki.lfaidata.foundation/download/attachments/61964495/18_IBM.pdf?version=1&amp;modificationDate=1657155776000&amp;api=v2">Slides</a>. <a href="https://www.youtube.com/watch?v=V70XXsPVrzg">Video</a>.</p></li>
</ol>
<h2 id="conferencejournal-papers">conference/journal papers</h2>
<hr />
<h3 id="section">2020</h3>
<ol type="1">
<li><p><strong>Tung D. Le</strong>, Gheorghe-Teodor Bercea, Tong Chen, Alexandre E Eichenberger, Haruki Imai, Tian Jin, Kiyokuni Kawachiya, Yasushi Negishi, Kevin O’Brien. 2020. <em>Compiling ONNX Neural Network Models Using MLIR</em>. arXiv:2008.08272, Retrieved from https://arxiv.org/abs/2008.08272v1</p></li>
<li><p>Haruki Imai, <strong>Tung D. Le</strong>, Yasushi Negishi, and Kiyokuni Kawachiya. 2020. <em>Acceleration of large deep learning training with hybrid GPU memory management of swapping and re-computing</em>. In Proceedings of the 2020 IEEE International Conference on Big Data (Big Data), December 10-13, 2020, Atlanta, GA, USA. IEEE, 1111-1116.</p></li>
</ol>
<h3 id="section-1">2019</h3>
<ol type="1">
<li><p>Haruki Imai, Samuel Matzek, <strong>Tung D. Le</strong>, Yasushi Negishi, Kiyokuni Kawachiya. 2019. <em>High Resolution Medical Image Segmentation Using Data-Swapping Method</em>. In: Shen D. et al. (eds) Medical Image Computing and Computer Assisted Intervention – MICCAI 2019. MICCAI 2019. Lecture Notes in Computer Science, Vol. 11766. Springer, Cham.</p></li>
<li><p><strong>Tung D. Le</strong>, Haruki Imai, Yasushi Negishi, Kiyokuni Kawachiya. 2019. <em>Automatic GPU Memory Management for Large Neural Models in TensorFlow</em>. In Proceedings of the 2019 ACM SIGPLAN International Symposium on Memory Management (ISMM 2019), June 2019, Phoenix, Arizona, USA. Association for Computing Machinery, New York, NY, USA, 1-13.</p></li>
<li><p>G. Janssen, V. Zolotov and <strong>Tung D. Le</strong>. 2019. <em>Large Data Flow Graphs in Limited GPU Memory</em>. In Proceedings of the 2019 IEEE International Conference on Big Data (Big Data), December 3-12, 2019, Los Angeles, CA, USA. IEEE, 1821-1830.</p></li>
<li><p>Yuki Ito, Haruki Imai, <strong>Tung Le Duc</strong>, Yasushi Negishi, Kiyokuni Kawachiya, Ryo Matsumiya, and Toshio Endo. 2019. <em>Profiling based out-of-core hybrid method for large neural networks.</em> In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming (PPoPP ’19), February 16-20, 2019, Washington, DC, USA. Association for Computing Machinery, New York, NY, USA, 399–400.</p></li>
</ol>
<h3 id="section-2">2018</h3>
<ol type="1">
<li><p>Minsik Cho, <strong>Tung D. Le</strong>, Ulrich A Finkler, Haruki Imai, Yasushi Negishi, Taro Sekiyama, Saritha Vinod, Vladimir Zolotov, Kiyokuni Kawachiya, David S. Kung, Hillery C. Hunter. 2018. <em>Large Model Support for Deep Learning in Caffe and Chainer</em>. 2018 SysML conference.</p></li>
<li><p><strong>Tung D. Le</strong>, Haruki Imai, Yasushi Negishi, Kiyokuni Kawachiya. 2018. <em>TFLMS: Large Model Support in TensorFlow by Graph Rewriting.</em> arXiv:1807.02037. Retrieved from https://arxiv.org/abs/1807.02037</p></li>
<li><p><strong>Tung D. Le</strong>, Taro Sekiyama, Yasushi Negishi, Haruki Imai, Kiyokuni Kawachiya. 2018. <em>Involving CPUs into Multi-GPU deep learning</em>. In Proceddings of the 2018 ACM/SPEC International Conference on Performance Engineering (ICPE ’18), March 2018, Berlin, Germany. Association for Computing Machinery, New York, NY, USA, 56–67.</p></li>
</ol>
<h3 id="section-3">2017</h3>
<ol type="1">
<li><strong>Tung D. Le</strong>, Taro Sekiyama, Yasushi Negishi, Haruki Imai, Kiyokuni Kawachiya. 2017. <em>Accelerating Multi-GPU Deep Learning by Collecting and Accumulating Gradients on CPUs</em>. SIG Technical Reports 2017-HPC-159(8), 1-8.</li>
</ol>
<h3 id="section-4">2016</h3>
<ol type="1">
<li><p><strong>Le-Duc Tung</strong>, Zhenjiang Hu. <em>Towards Systematic Parallelization of Graph Transformations over Pregel</em>. International Journal of Parallel Program. 45, 2 (April 2017), 320–339.</p></li>
<li><p>Chong Li, <strong>Le-Duc Tung</strong>, Xiaodong Meng, Zhenjiang Hu. 2016. <em>Derivation of parallel-efficient structural recursive functions from declarative graph queries</em>. In Proceedings of the 31st Annual ACM Symposium on Applied Computing (SAC ’16), April 2016, Pisa, Italy. Association for Computing Machinery, New York, NY, USA, 1922–1925.</p></li>
</ol>
<h3 id="section-5">2015</h3>
<ol type="1">
<li><p><strong>Le-Duc Tung</strong>, Zhenjiang Hu. 2015. <em>Towards Systematic Parallelization of Graph Trans- formations over Pregel</em>. In Proceedings of the 8th International Symposium on High-level Parallel Programming and Applications (HLPP 2015), July 2-3, 2015, Pisa, Italy.</p></li>
<li><p><strong>Le-Duc Tung</strong>, Zhenjiang Hu. <em>Pregel meets UnCAL: a Systematic Framework for Transforming Big Graphs</em>. In Proceedings of the 2015 31st International Conference on Data Engineering Workshops (ICDE2015), April 13-17, 2015, Seoul, South Korea. IEEE, 250-254.</p></li>
</ol>
<h3 id="section-6">2013</h3>
<ol type="1">
<li><p><strong>Le-Duc Tung</strong>, Nguyen-Van Quyet, Zhenjiang Hu. 2013. <em>Efficient Query Evaluation on Distributed Graphs with Hadoop Environment</em>. In Proceedings of the Fourth International Symposium on Information and Communication Technology (SoICT ’13), December 5-6, 2013, Da Nang, Vietnam. Association for Computing Machinery, New York, NY, USA, 311—319,</p></li>
<li><p>Nguyen-Van Quyet, <strong>Le-Duc Tung</strong>, Zhenjiang Hu. <em>Minimizing Data Transfers for Regular Reachability Queries on Distributed Graphs</em>. In Proceedings of the Fourth International Symposium on Information and Communication Technology (SoICT ’13), December 5-6, 2013, Da Nang, Vietnam. Association for Computing Machinery, New York, NY, USA, 325-334.</p></li>
</ol>
<h3 id="section-7">2012</h3>
<ol type="1">
<li><strong>D. T. Le</strong>, H. D. Nguyen, T. A. Pham, H. H. Ngo and M. T. Nguyen. 2012. <em>An Intermediate Library for Multi-GPUs Computing Skeletons</em>. In Proceedings of the 2012 IEEE RIVF International Conference on Computing &amp; Communication Technologies, Research, Innovation, and Vision for the Future (RIVF ’12), March 2012, Ho Chi Minh, Vietnam. IEEE, 1-6.</li>
</ol>
<h2 id="contact">contact</h2>
<hr />
<p>Office address: <a href="https://goo.gl/maps/mYFiyX95DBXEELrLA">23rd floor, river side, IBM Japan headquarters building</a>. <br> Telephone number: +81-(80)-5915-1439 <br> E-mail: tung@jp.ibm.com</p>
<hr />
<p>This page was generated by <a href=https://pandoc.org>Pandoc</a> from <a href=https://daringfireball.net/projects/markdown/syntax>Markdown</a>. Its source code is available <a href=https://github.com/tungld/tungld.github.io>here</a>. Latest update on 29 July, 2025.
</p>
</body>
</html>
